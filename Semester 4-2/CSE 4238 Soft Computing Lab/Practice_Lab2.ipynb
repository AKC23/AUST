{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ILa4hbOxdnJ"
      },
      "source": [
        "## One Layer FNN with Sigmoid Activation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJ1dVc9mgbN8"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as dsets"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXVIydDCxDPS"
      },
      "source": [
        "- **Input dimension:**\n",
        "  - Size of image: $28 \\times 28 = 784$\n",
        "\n",
        "- **Output dimension: 10**\n",
        "  - 0, 1, 2, 3, 4, 5, 6, 7, 8, 9"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5hVijghPqz0"
      },
      "source": [
        "# Hyperparameters\n",
        "\n",
        "batch_size = 100\n",
        "num_iters = 3000\n",
        "input_dim = 28*28 # num_features = 784\n",
        "num_hidden = 100 # num of hidden nodes\n",
        "output_dim = 10\n",
        "\n",
        "learning_rate = 0.1  # More power so we can learn faster! previously it was 0.001\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4R6x4MvEsOT"
      },
      "source": [
        "### Loading MNIST Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUumuKA-cahD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be59300d-8b49-4607-b2d4-bf4258d26ced"
      },
      "source": [
        "'''\n",
        "LOADING DATASET\n",
        "'''\n",
        "train_dataset = dsets.MNIST(root='./data',\n",
        "                            train=True,\n",
        "                            transform=transforms.ToTensor(),  # Normalize the image to [0-1] from [0-255]\n",
        "                            download=True)\n",
        "\n",
        "test_dataset = dsets.MNIST(root='./data',\n",
        "                           train=False,\n",
        "                           transform=transforms.ToTensor())\n",
        "\n",
        "'''\n",
        "MAKING DATASET ITERABLE\n",
        "'''\n",
        "num_epochs = num_iters / (len(train_dataset) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True)   # It's better to shuffle the whole training dataset!\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 134541825.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 35030565.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 38707625.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 22412386.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmkMVvf8CLHf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c5b9114-4811-4726-c6c1-092a409f7df1"
      },
      "source": [
        "print(len(train_dataset))\n",
        "print(len(test_dataset))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60000\n",
            "10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Isz6lbl4Iovx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8af9a47-05f3-4eaf-b17e-e9008cc49812"
      },
      "source": [
        "# One Image Size\n",
        "print(train_dataset[0][0].size())\n",
        "print(train_dataset[0][0].numpy().shape)\n",
        "# First Image Label\n",
        "print(train_dataset[0][1])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 28, 28])\n",
            "(1, 28, 28)\n",
            "5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQB8tvNUQZop"
      },
      "source": [
        "<div align=\"center\">\n",
        "<img src=\"https://drive.google.com/uc?id=1mn8G92moF0MqXhD0J-M7cPidCYXR0hHS\" width=\"680\" height=\"380\">\n",
        "</div>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRm3MYkW8QVU"
      },
      "source": [
        "### Step #1 : Design your model using class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mydzEXpeu7G"
      },
      "source": [
        "class NeuralNetworkModel(nn.Module):\n",
        "    def __init__(self, input_size, num_classes, num_hidden, nh_2, nh_3):\n",
        "        super().__init__()\n",
        "        ### 1st hidden layer\n",
        "        self.linear_1 = nn.Linear(input_size, num_hidden)\n",
        "        self.linear_2 = nn.Linear(num_hidden, nh_2)\n",
        "        self.linear_3 = nn.Linear(nh_2, nh_3)\n",
        "\n",
        "        ### Non-linearity\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        ### Output layer\n",
        "        self.linear_out = nn.Linear(nh_3, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Linear layer\n",
        "        out  = self.linear_1(x)\n",
        "        # Non-linearity\n",
        "        out = self.sigmoid(out)\n",
        "        out = self.linear_2(out)\n",
        "        out = self.tanh(out)\n",
        "        out = self.linear_3(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        # Linear layer (output)\n",
        "        probas  = self.linear_out(out)\n",
        "        return probas"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIfiAaZB1rJz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56ffd642-eac2-49c6-af62-5fb598f1cb9f"
      },
      "source": [
        "'''\n",
        "INSTANTIATE MODEL CLASS\n",
        "'''\n",
        "model = NeuralNetworkModel(input_size = input_dim,\n",
        "                           num_classes = output_dim,\n",
        "                           num_hidden = 150, nh_2=200, nh_3=100)\n",
        "# To enable GPU\n",
        "model.to(device)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NeuralNetworkModel(\n",
              "  (linear_1): Linear(in_features=784, out_features=150, bias=True)\n",
              "  (linear_2): Linear(in_features=150, out_features=200, bias=True)\n",
              "  (linear_3): Linear(in_features=200, out_features=100, bias=True)\n",
              "  (sigmoid): Sigmoid()\n",
              "  (tanh): Tanh()\n",
              "  (relu): ReLU()\n",
              "  (linear_out): Linear(in_features=100, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdrDJPOKzdSp"
      },
      "source": [
        "###Step #2 : Construct loss and optimizer\n",
        "\n",
        "Unlike linear regression, we do not use MSE here, we need Cross Entropy Loss to calculate our loss before we backpropagate and update our parameters.\n",
        "\n",
        "`criterion = nn.CrossEntropyLoss() `\n",
        "\n",
        "It does 2 things at the same time.\n",
        "\n",
        "1. Computes softmax ([Logistic or Sigmoid]/softmax function)\n",
        "2. Computes Cross Entropy Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GM2q_XGHzcta"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2Hb_JQ6AUok"
      },
      "source": [
        "###Step #3 : Training: forward, loss, backward, step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3Jb4vhRZI9p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3c00048-c123-4ba9-ac8c-715c024f846a"
      },
      "source": [
        "'''\n",
        "TRAIN THE MODEL\n",
        "'''\n",
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "        images = images.view(-1, 28*28).to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass to get output/logits\n",
        "        outputs = model(images)\n",
        "\n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        iter += 1\n",
        "\n",
        "        if iter % 500 == 0:\n",
        "            # Calculate Accuracy\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for images, labels in test_loader:\n",
        "\n",
        "                images = images.view(-1, 28*28).to(device)\n",
        "\n",
        "                # Forward pass only to get logits/output\n",
        "                outputs = model(images)\n",
        "\n",
        "                # Get predictions from the maximum value\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "                # Total number of labels\n",
        "                total += labels.size(0)\n",
        "\n",
        "\n",
        "                # Total correct predictions\n",
        "                if torch.cuda.is_available():\n",
        "                    correct += (predicted.cpu() == labels.cpu()).sum()\n",
        "                else:\n",
        "                    correct += (predicted == labels).sum()\n",
        "\n",
        "            accuracy = 100 * correct.item() / total\n",
        "\n",
        "            # Print Loss\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 500. Loss: 0.7409985065460205. Accuracy: 76.56\n",
            "Iteration: 1000. Loss: 0.36048170924186707. Accuracy: 88.84\n",
            "Iteration: 1500. Loss: 0.23520435392856598. Accuracy: 89.73\n",
            "Iteration: 2000. Loss: 0.2882601022720337. Accuracy: 91.19\n",
            "Iteration: 2500. Loss: 0.4650675654411316. Accuracy: 91.14\n",
            "Iteration: 3000. Loss: 0.17210297286510468. Accuracy: 92.71\n"
          ]
        }
      ]
    }
  ]
}